{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from __future__ import division\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import part1_utils\n",
    "reload(part1_utils)\n",
    "from part1_utils import test_hypothesis, test_quadLoss, plot_1D, plot_f, load_forest_data, forest_test_evaluate,\\\n",
    "                    load_death_rate_training, death_rate_evaluate\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Supervised Machine Learning </h1>\n",
    "In this project, we are going to get our hands dirty with some of the basic building blocks in supervised machine learning. All together, a supervised learning problem is defined by three components: <it>inputs and outputs, a hypothesis class,</it> and <it>a loss function.</it> Let's go over each of these.\n",
    "\n",
    "<h3>Inputs and Outputs</h3>\n",
    "In each application of ML, one of the most important questions to answer is what type problem you want to solve.  When we were categorizing images, our <it>inputs</it> were color images and our <it>outputs</it> were image categories (dog, cat, chair...etc). The inputs and outputs determine what kind of dataset we should collect. Next, we'll look at <it>regression</it> where the output and input is a single real number.\n",
    "\n",
    "<h3>Hypothesis Class</h3>\n",
    "The actual work of classifing images above was done by the neural network that you loaded above. More generally, we use a <it>predictor</it> or <it>hypothesis</it> to represent the mapping from inputs to outputs. The <it>hypothesis class</it> is the group of all input->output mappings we will consider as outputs of our supervised machine learning algorithm. The term <it>neural network</it> actually refers to a (somewhat complicated) hypothesis class that has led to recent progress in image and audio classification. We'll get to that tomorrow. In the next exercise, we'll focus on the simpler hypothesis class of <it>linear</it> functions.\n",
    "\n",
    "<h3>Loss Function</h3>\n",
    "Once we know what inputs and outputs to consider and what predictors we can use, we need to know how to pick the 'best' predictor. There are lots of ways to classify images into categories but many of these will be nonsense or random. The <it>loss function</it> is what defines what it means to make a prediction correctly. It takes as input a predicted output and a labelled, or true, output and tells you how much of a mistake the predictor made. If the loss is high, then the prediction is a bad prediction. In image classification, the loss function takes in a predicted label and a true label. It gives back one if the two labels are different and 0 if they are the same. When we do regression, we'll use the squared distance between the predicted number and the true number as the loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Implement a function that takes as input the constants (w, w_0) and an input (x) for the line y = w * x + w_0\n",
    "## and compute the corresponding y value\n",
    "def hypothesis(w, w0, x):\n",
    "    \"\"\"Replace this with your code\"\"\"\n",
    "    return 0\n",
    "\n",
    "print \"testing hypothesis\"\n",
    "result = test_hypothesis(hypothesis)\n",
    "print \"Success?:\\t{}\".format(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've implemented a hypothesis function, we can generate some data! We'll create some random x values and a random set of \"true\" weights for our function. We can use this to compute the labels for these points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## generate some random x values between 0 and 1\n",
    "x_vals = np.random.rand(100)\n",
    "w_true, w0_true = (3, 1)\n",
    "print w_true, w0_true\n",
    "y_vals = []\n",
    "for x in x_vals:\n",
    "    y_vals.append(hypothesis(w_true, w0_true, x))\n",
    "\n",
    "## Compare that plot with a plot of the line\n",
    "plot_1D(w_true, w0_true, x_vals, y_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We usually observe 'noisy' data --- data that doesn't exactly fit our model. This could be because of faulty sensors or just random chance. Next, you'll write a function that will let us generate data from a noisy version of our hypothesis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def noisy_hypothesis(w, w0, x):\n",
    "    \"\"\"\n",
    "    compute y = w * x + w0 + e\n",
    "    e is a random number generated with np.random.normal()\n",
    "    \"\"\"\n",
    "    \"\"\"Replace with your code\"\"\"\n",
    "    return 0\n",
    "\n",
    "## generate some noisy evaluations\n",
    "y_vals_noisy = []\n",
    "for x in x_vals:\n",
    "    y_vals_noisy.append(noisy_hypothesis(w_true, w0_true, x))\n",
    "\n",
    "## create a scatter plot of the noisy data\n",
    "plt.scatter(x_vals, y_vals_noisy)\n",
    "## plotting with the line in the background\n",
    "plot_1D(w_true, w0_true, x_vals, y_vals_noisy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In regression, we start with data like this and we have to figure out the weights that generated the data. Next, we have some data that we generated with the same process as above. Using the plotting code we have given you, try to figure out the correct weights for yourself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Weight Fitting 0\n",
    "x_0 = np.load('x_0.npy')\n",
    "y_0 = np.load('y_0.npy')\n",
    "\n",
    "## Replace with your estimates\n",
    "w_0, w0_0 = 1, 0\n",
    "\n",
    "plot_1D(w_0, w0_0, x_0, y_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Weight Fitting 1\n",
    "x_1 = np.load('x_1.npy')\n",
    "y_1 = np.load('y_1.npy')\n",
    "\n",
    "## Replace with your estimates\n",
    "w_1, w0_1 = 1, 0\n",
    "\n",
    "plot_1D(w_1, w0_1, x_1, y_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Weight Fitting 2\n",
    "x_2 = np.load('x_2.npy')\n",
    "y_2 = np.load('y_2.npy')\n",
    "\n",
    "## Replace with your estimates\n",
    "w_2, w0_2 = 1, 0\n",
    "\n",
    "plot_1D(w_2, w0_2, x_2, y_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to figure out a better way to tell if our guessed weights are good (instead of just using our eyes). This is the job of the <it>loss function</it>. It takes two inputs, a guess and a true value, and tells us how much our guess missed by. In linear regression, we use squared distance to measure the loss associated with a guess. If our guess is close, but not quite right, we'll get a small penalty. However, if we miss by a lot we'll pay a big price. Next, we'll write a function that'll let us evaluate how well a set of weights does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Implement a function that takes as input a predicted y value y_guess and a true output value y_true\n",
    "## and returns the loss that guess would incur: L(y_guess, y_true) = (y_guess - y_true)**2\n",
    "def quad_loss(y_guess, y_true):\n",
    "    \"\"\"Replace with your code\"\"\"\n",
    "    return 0\n",
    "\n",
    "## Implement a function that takes as input weights, a list of inputs, and a list of labels (desired outputs)\n",
    "## and computes the loss over the whole dataset\n",
    "## Hint: you should be able to re-use your answers from before\n",
    "def dataset_loss(w, w0, x_vals, y_vals):\n",
    "    \"\"\"Replace with your code\"\"\"\n",
    "    ## loop through the dataset\n",
    "    loss = 0\n",
    "    for x, y in zip(x_vals, y_vals):\n",
    "        \"\"\"replace with your code\"\"\"\n",
    "        loss += 0\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Let's test out how well your estimates from the questions above did\n",
    "l_0 = dataset_loss(w_0, w0_0, x_0, y_0)\n",
    "print \"Loss for question 0: {}\".format(l_0)\n",
    "\n",
    "l_1 = dataset_loss(w_1, w0_1, x_1, y_1)\n",
    "print \"Loss for question 1: {}\".format(l_1)\n",
    "\n",
    "l_2 = dataset_loss(w_2, w0_2, x_2, y_2)\n",
    "print \"Loss for question 2: {}\".format(l_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the dataset loss function you implemented, see if you can find weights for the regression problems above that do better than the weights you found by eyeballing the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Replace with your estimates\n",
    "w_0, w0_0 = 1, 0\n",
    "plot_1D(w_0, w0_0, x_0, y_0)\n",
    "\n",
    "print \"Loss:\\t{}\".format(dataset_loss(w_0, w0_0, x_0, y_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Replace with your estimates\n",
    "w_1, w0_1 = 1, 0\n",
    "plot_1D(w_1, w0_1, x_1, y_1)\n",
    "\n",
    "print \"Loss:\\t{}\".format(dataset_loss(w_1, w0_1, x_1, y_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Replace with your estimates\n",
    "w_2, w0_2 = 1, 0\n",
    "plot_1D(w_2, w0_2, x_2, y_2)\n",
    "\n",
    "print \"Loss:\\t{}\".format(dataset_loss(w_2, w0_2, x_2, y_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Optimization </h1>\n",
    "In this part of the exercise, we'll learn the basics of how we can find the best weights using <it>optimization</it>. Optimization in mathematics is the problem of finding the smallest (or biggest) value of some function. So, if we have a function <b>f(x)</b>, then optimizing this function will find a value for <b>x</b> so that <b>f(x)</b> is as small as possible. \n",
    "\n",
    "The world is full of problems that look like optimization problems. For example, figuring out how to get to school quickly is an optimization (find a road that minimizes the total travel time). Optimization is one of the biggest tools in artificial intelligence.\n",
    "\n",
    "We will use optimization to solve linear regression by finding a set of weights that minimize the loss we incur across the entire data set. \n",
    "\n",
    "First, we'll start with something simpler: minimizing a parabola. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## First, we need to implement a parabola\n",
    "def parabola(a, b, c, x):\n",
    "    \"\"\"\n",
    "    compute the quadratic function y(x) = ax^2 + bx + c\n",
    "    \"\"\"\n",
    "    \"\"\"replace with your code\"\"\"\n",
    "    return 0\n",
    "\n",
    "## some initial values for a, b, c\n",
    "a, b, c = 4, -4.27, 2\n",
    "f = lambda x: parabola(a, b, c, x)\n",
    "plot_f(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest way to minimize a function like this is to use brute force. We can't try every value of x, but we can try enough that we'll be close to the right answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def minimize_discrete_1(f, x_min, x_max, delta):\n",
    "    x = x_min\n",
    "    ## we'll keep track of the best values we have seen so far\n",
    "    bestx = x\n",
    "    besty = f(x)\n",
    "    ## as long as x is smaller than the max\n",
    "    while x < x_max:\n",
    "        ## increase x by delta and store the value in x_new\n",
    "        \"\"\"replace with your code\"\"\"\n",
    "        x_new = x\n",
    "        y = f(x_new)\n",
    "        ## figure out if this value is better\n",
    "        \"\"\"replace with your code\"\"\"\n",
    "        if 0:\n",
    "            bestx = x_new\n",
    "            besty = y\n",
    "        x = x_new\n",
    "    return bestx, besty\n",
    "opt_x, opt_y = minimize_discrete_1(f, 0, 1, 1e-1)\n",
    "plot_f(f, [opt_x], [opt_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## If we zoom in, we can see that this answer is not perfect\n",
    "plot_f(f, [opt_x], [opt_y], (.4, .6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## We can do better by making the step size smaller\n",
    "opt_x, opt_y = minimize_discrete_1(f, 0, 1, 1e-2)\n",
    "plot_f(f, [opt_x], [opt_y], (.4, .6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check how accurate this is by taking advantage of the fact that minimizing a quadratic function can be done analytically. From this, we can compare the solution we get from our optimizer with the exact minimum for a bunch of different step sizes. In the next cell, we use some simple timing code to generate a plot of running time vs accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def minimize_analytic(a, b, c):\n",
    "    return -b / (2*a)\n",
    "\n",
    "## But eventually doing better will take longer\n",
    "deltas = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6]\n",
    "\n",
    "min_x = minimize_analytic(a, b, c)\n",
    "min_y = f(min_x)\n",
    "\n",
    "times, subopts = [], []\n",
    "\n",
    "for delta in deltas:\n",
    "    start = time.time()\n",
    "    x, y = minimize_discrete_1(f, 0, 1, delta)\n",
    "    time_taken = time.time() - start\n",
    "    times.append(np.log(time_taken))\n",
    "    \n",
    "    subopt = y - min_y\n",
    "    ## making sure we don't feed a 0 into the log\n",
    "    subopts.append(np.log(np.maximum(subopt, 1e-10)))\n",
    "    print \"delta: {}\\ttime: {}\\t x distance: {}\\ty distance: {}\".format(\n",
    "        delta, time_taken, np.abs(x - min_x), np.abs(y - min_y))\n",
    "    \n",
    "plt.scatter(subopts, times)\n",
    "plt.ylabel('runtime log(seconds)')\n",
    "plt.xlabel('accuracy log(y - min_y)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our baseline. Our goal is to figure out how to get the same level of optimality in less time or to do better in the same amount of time. The first thing we can do is figure out if we can terminate early. This is a parabola, so, if the function is <it>increasing</it> at the current value, then it will never go down again. We can check this and use this fact to terminate our search early."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_increasing_discrete(f, x, delta):\n",
    "    \"\"\"Return True if the next value checked in minimize_discrete \n",
    "    will increase\"\"\"\n",
    "    \"\"\"Replace with your code\"\"\"\n",
    "    return 0\n",
    "\n",
    "def minimize_discrete_2(f, x_min, x_max, delta):\n",
    "    \"\"\"\n",
    "    modify your implementation of minimize_discrete_1 to use check_increasing_discrete\n",
    "    and terminate early\n",
    "    \"\"\"\n",
    "    \"\"\"Replace with your code\"\"\"\n",
    "    return x_min, f(x_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will see how much better we have done! One important thing to pay attention to is that we want to make sure we can minimize <i>many</i> different functions. We'll use some random values for a, b, and c to test our improvement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "times_1 = []\n",
    "subopts_1 = []\n",
    "\n",
    "times_2 = []\n",
    "subopts_2 = []\n",
    "\n",
    "speedups = []\n",
    "delta_list = []\n",
    "\n",
    "## use fewer delta values so it doesn't take forever :)\n",
    "deltas = [1e-1, 1e-2, 1e-3, 1e-4]\n",
    "for i in range(100):\n",
    "    ## need a to be positive for minimization to make sense\n",
    "    a = np.abs(np.random.normal())\n",
    "    b = np.random.normal()\n",
    "    c = np.random.normal()\n",
    "    f = lambda x: parabola(a, b, c, x)\n",
    "    min_x = minimize_analytic(a, b, c)\n",
    "    min_y = f(min_x)\n",
    "    for delta in deltas:\n",
    "        t_0 = time.time()\n",
    "        x_1, y_1 = minimize_discrete_1(f, -2, 2, delta)\n",
    "        t_1 = time.time()\n",
    "        x_2, y_2 = minimize_discrete_2(f, -2, 2, delta)\n",
    "        t_2 = time.time()\n",
    "        subopt = np.maximum(np.abs(y_1 - min_y), 1e-10)\n",
    "        subopts_1.append(np.log(subopt))\n",
    "        times_1.append(np.log(t_1 - t_0))\n",
    "        \n",
    "        subopt = np.maximum(np.abs(y_2 - min_y), 1e-10)\n",
    "        subopts_2.append(np.log(subopt))\n",
    "        times_2.append(np.log(t_2 - t_1))\n",
    "        \n",
    "        speedups.append((t_1 - t_0) - (t_2 - t_1))\n",
    "        delta_list.append(delta)\n",
    "        #print x_1, x_2, min_x, min_y, t_1 - t_0 - t_2 + t_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## plot the results side-by-side\n",
    "fig, axes = plt.subplots(1, 2, sharey=True)\n",
    "plt.sca(axes[0])\n",
    "plt.scatter(subopts_1, times_1)\n",
    "plt.sca(axes[1])\n",
    "plt.scatter(subopts_2, times_2)\n",
    "plt.ylabel('runtime log(seconds)')\n",
    "plt.xlabel('accuracy log(y - min_y)')\n",
    "\n",
    "\n",
    "fig1, axes1 = plt.subplots(1, 2)\n",
    "speedups = np.array(speedups)\n",
    "plt.sca(axes1[0])\n",
    "sns.distplot(speedups)\n",
    "plt.sca(axes1[1])\n",
    "plt.scatter(np.log(delta_list), speedups)\n",
    "\n",
    "print \"Average Speedup: {}\\tMax: {}\".format(np.mean(speedups), np.max(speedups))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that's not good, we got faster in some cases, but slower in others. The reason is that we are always doing fewer checks, but each iteration of the search takes longer. This is a common tradeoff when trying to write optimization code. Usually we can make our algorithm 'smarter' so it takes fewer steps ---  but that means that each step will take more time. \n",
    "\n",
    "Time to try something new! What's really making us slow up above is that we only take a small step each iteration. If we want good accuracy we need to make the step size small. What we'll try next is to take different sized steps at each iteration. The intuition we'll use is this: if the function is increasing rapidly, then we'll take a big step; but, if it is increasing slowly, then we are close to the bottom and we should take a small step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## First, we'll need a different convergence check to see if we are at the minimum. \n",
    "## This time, we'll check explicitly to see if the value is a minimum by comparing against \n",
    "## its neighbors. \n",
    "def is_min_discrete(f, x, delta):\n",
    "    \"\"\"Compute the function value at x+delta and x-delta and compare those values\n",
    "    with f(x). Return true if f is increasing in both directions.\"\"\"\n",
    "    v = f(x)\n",
    "    v_lo = f(x-delta)\n",
    "    v_hi = f(x+delta)\n",
    "    return v <= v_lo and v <= v_hi\n",
    "\n",
    "## Next, we'll compute the amount to increase (or decrease) x.\n",
    "## The step size that we'll use is the average slope. This is (f(x+delta) - f(x-delta))/(2delta)\n",
    "## We'll use the hyperparameter alpha to control how sensitive we are to the slope\n",
    "## alpha is a hyperparameter that allows us to control how quickly we will move\n",
    "## NOTE: this is positive when f is increasing and negative when f is decreasing\n",
    "## so we always want to move in the opposite direction of this value\n",
    "def step_size(f, x, alpha, delta):\n",
    "    v = f(x)\n",
    "    v_lo = f(x-delta)\n",
    "    v_hi = f(x+delta)\n",
    "    return alpha * (v_hi - v_lo)/(2*delta)\n",
    "\n",
    "## Next, we can write our third version of minimize_discrete\n",
    "def minimize_discrete_3(f, x_init, alpha, delta):\n",
    "    \"\"\"\n",
    "    modify your implementation of minimize_discrete_2 to use is_min_discrete for termination\n",
    "    and step_size to use an adaptive step size. \n",
    "    \"\"\"\n",
    "    \"\"\"Replace with your code\"\"\"\n",
    "    return x_init, f(x_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "times_1 = []\n",
    "subopts_1 = []\n",
    "\n",
    "times_3 = []\n",
    "subopts_3 = []\n",
    "\n",
    "speedups_3 = []\n",
    "delta_list = []\n",
    "\n",
    "## use fewer delta values so it doesn't take forever :)\n",
    "deltas = [1e-1, 1e-2, 1e-3, 1e-4]\n",
    "for i in range(100):\n",
    "    ## need a to be positive for minimization to make sense\n",
    "    a = np.abs(np.random.normal())\n",
    "    b = np.random.normal()\n",
    "    c = np.random.normal()\n",
    "    f = lambda x: parabola(a, b, c, x)\n",
    "    min_x = minimize_analytic(a, b, c)\n",
    "    min_y = f(min_x)\n",
    "    start = time.time()\n",
    "    x_3, y_3 = minimize_discrete_3(f, 0, 1e-1, 1e-5)\n",
    "    t_3 = time.time() - start\n",
    "    subopt = np.maximum(np.abs(y_3 - min_y), 1e-10)\n",
    "    #print x_3, y_3, t_3, subopt\n",
    "    subopts_3.append(np.log(subopt))\n",
    "    times_3.append(np.log(t_3))\n",
    "    for delta in deltas:\n",
    "        t_0 = time.time()\n",
    "        x_1, y_1 = minimize_discrete_1(f, -2, 2, delta)\n",
    "        t_1 = time.time()\n",
    "\n",
    "        subopt = np.maximum(np.abs(y_1 - min_y), 1e-10)\n",
    "        subopts_1.append(np.log(subopt))\n",
    "        times_1.append(np.log(t_1 - t_0))\n",
    "        \n",
    "        #print x_1, y_1, t_1 - t_0, subopt\n",
    "        speedups_3.append((t_1 - t_0) - (t_3))\n",
    "        delta_list.append(delta)\n",
    "        #print x_1, x_2, min_x, min_y, t_1 - t_0 - t_2 + t_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## plot the results side-by-side\n",
    "fig, axes = plt.subplots(1, 2, sharey=True)\n",
    "plt.sca(axes[0])\n",
    "plt.scatter(subopts_1, times_1)\n",
    "plt.sca(axes[1])\n",
    "plt.scatter(subopts_3, times_3)\n",
    "plt.ylabel('runtime log(seconds)')\n",
    "plt.xlabel('accuracy log(y - min_y)')\n",
    "\n",
    "\n",
    "fig1, axes1 = plt.subplots(1, 1)\n",
    "speedups_3 = np.array(speedups_3)\n",
    "#speedups_3_clipped = speedups_3.copy()\n",
    "#speedups_3_clipped[speedups_3_clipped > .05] = .05\n",
    "#speedups_3_clipped[speedups_3_clipped < -.05] = -.05\n",
    "plt.sca(axes1)\n",
    "sns.distplot(speedups_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hooray! This looks a lot better. First, we can see that we pretty much always do perfectly on accuracy. The top left graph is entirely clustered on a single suboptimality value. The next thing is that we almost never get a worse running time. There are a couple outliers, most values in the histogram should be positive (or at least close to 0). Futhermore, we sometimes get very large speed ups (a tenth of a second). \n",
    "\n",
    "We've now arrived at a discretized version of a classic optimization method called <it>gradient descent</it>. A gradient is a tool from calculus that allows us to measure the slope of a function at a particular point. For now, we can just think about it as the behavior of step_size with a very very small delta. For many functions, we can compute gradients exactly. For the parabola <b>f(x) ax^2 + bx + c</b> the gradient is <b>df(x) = 2ax + b</b>. We can use this to implement and test gradient descent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## First, define the gradient of a parabola\n",
    "def d_quad_dx(a, b, c, x):\n",
    "    return 2*a*x + b\n",
    "\n",
    "## We will also use the gradient for convergence. \n",
    "## If the next step we will take is going to be small, then we \n",
    "## know we are close to optimal. We'll take the derivative of f as an input.\n",
    "## We will say that we have found an answer if the magnitude of the derivative |df(x)| \n",
    "## is less than epsilon\n",
    "def is_min_grad(x, df, eps):\n",
    "    #print '\\r', np.linalg.norm(df(x)), \n",
    "    return np.linalg.norm(df(x)) <= eps\n",
    "\n",
    "## Now we're ready to implement gradient descent. Modify your\n",
    "## implementation of minimize_discrete_3 to use a derivative\n",
    "## for the step size and convergence check. To keep things simple, we'll\n",
    "## take df, the derivative function, as an input. \n",
    "def gradient_descent(f, df, x_init, alpha, eps):\n",
    "    \"\"\"replace with your code\"\"\"\n",
    "    return x_init, f(x_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "speedups = []\n",
    "\n",
    "for i in range(100):\n",
    "    ## need a to be positive for minimization to make sense\n",
    "    a = np.abs(np.random.normal())\n",
    "    b = np.random.normal()\n",
    "    c = np.random.normal()\n",
    "    f = lambda x: parabola(a, b, c, x)\n",
    "    df = lambda x: d_quad_dx(a, b, c, x)\n",
    "    min_x = minimize_analytic(a, b, c)\n",
    "    min_y = f(min_x)\n",
    "    t_0 = time.time()\n",
    "    x_0, y_0 = minimize_discrete_3(f, 0, 1e-1, 1e-5)\n",
    "    t_1 = time.time()\n",
    "    x_1, y_1 = gradient_descent(f, df, 0, 1e-1, 1e-8)\n",
    "    t_2 = time.time()\n",
    "    time_discrete = t_1 - t_0\n",
    "    time_analytic = t_2 - t_1\n",
    "    speedup = time_discrete - time_analytic\n",
    "    speedups.append(speedup)\n",
    "    \n",
    "fig1, axes1 = plt.subplots(1, 1)\n",
    "speedups = np.array(speedups)\n",
    "plt.sca(axes1)\n",
    "sns.distplot(speedups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good news again! In most trials we should see a small speed improvement and in a few we see a large improvement. The small improvment overall is because we've actually made each iteration faster. Computing the exact derivative is faster than computing an approximation. This speeds up the convergence check and the step size computation. The large improvement come from places where the approximation caused the algorithm to make mistakes --- using the exact derivative fixes this problem.\n",
    "\n",
    "Now, we can go back to our linear regression problem and use the gradient descent function we wrote to fit some lines! We just need to figure out what to make <b>f</b> and <b>x</b>. Remember that our goal is to determine a linear function that can represent a set of (x, y) pairs. We use a loss function to measure how good each guess is. Thus, we can use the loss summed across the dataset (dataset_loss from before) as our function to minimize. We are minimizing this over the different choices of w0 and w."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## First we'll try this out with a particular set of weights\n",
    "w, w0 = 2.3, -1.4\n",
    "\n",
    "x_vals = np.random.rand(100)\n",
    "## compute y values for each x value\n",
    "y_vals = np.array([hypothesis(w, w0, x) for x in x_vals])\n",
    "## noise to add to our data\n",
    "e_vals = np.random.normal(size=100)\n",
    "noisy_y_vals = y_vals + e_vals\n",
    "\n",
    "## plot the clean data\n",
    "plot_1D(w, w0, x_vals, y_vals)\n",
    "\n",
    "## plot the noisy data\n",
    "plot_1D(w, w0, x_vals, noisy_y_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## We need to compute the gradient of dataset_loss with respect to the weights. \n",
    "## To do this, one useful trick is that if we have a function that is a sum of other functions,\n",
    "## then its derivative is the sum of the derivatives. \n",
    "## In other words: if f(x) = g(x) + h(x), then df(x) = dh(x) + dg(x).\n",
    "## dataset_loss is a sum of quadratic function, so we can use d_quad_loss_dw to compute this gradient\n",
    "\n",
    "def d_quad_loss_dw(w, w0, x, y):\n",
    "    y_guess = w * x + w0\n",
    "    return 2 * (y_guess - y) * np.array([x, 1])\n",
    "\n",
    "## Note: the gradient has two dimensions (because there are two variables that describe our weights [w, w0]).\n",
    "## Make sure you are dealing with that correctly!\n",
    "def d_dataset_loss_dw(w, w0, x_vals, y_vals):\n",
    "    grad = np.array([0., 0.])\n",
    "    for x, y in zip(x_vals, y_vals):\n",
    "        \"\"\"Replace with your code\"\"\"\n",
    "        grad += 0\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Now we can use our gradient descent function to fit weights to our data!\n",
    "\n",
    "## This time the variable has two dimensions to it, instead of 1\n",
    "w_init = np.array([0, 0])\n",
    "\n",
    "## First we'll try fitting to the clean data to make sure everything is working correctly\n",
    "f_clean = lambda x: dataset_loss(x[0], x[1], x_vals, y_vals)\n",
    "df_clean = lambda x: d_dataset_loss_dw(x[0], x[1], x_vals, y_vals)\n",
    "\n",
    "(w_clean, w0_clean), l = gradient_descent(f_clean, df_clean, w_init, 1e-3, 1e-6)\n",
    "\n",
    "plot_1D(w_clean, w0_clean, x_vals, y_vals)\n",
    "\n",
    "print \"True Weights: {}, {}\\t Inferred Weights: {}, {}\".format(w, w0, w_clean, w0_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Now we can try it with the noisy data\n",
    "f_noisy = lambda x: dataset_loss(x[0], x[1], x_vals, noisy_y_vals)\n",
    "df_noisy = lambda x: d_dataset_loss_dw(x[0], x[1], x_vals, noisy_y_vals)\n",
    "\n",
    "(w_noisy, w0_noisy), l = gradient_descent(f_noisy, df_noisy, w_init, 1e-3, 1e-8)\n",
    "\n",
    "plot_1D(w_noisy, w0_noisy, x_vals, noisy_y_vals)\n",
    "\n",
    "\n",
    "print \"True Weights: {}, {}\\t Inferred Weights: {}, {}\".format(w, w0, w_noisy, w0_noisy)\n",
    "print \"True Weights Loss: {}\\t Inferred Weights Loss: {}\".format(f_noisy([w, w0]), f_noisy([w_noisy, w0_noisy]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1> (Over-)Fitting Polynomials </h1> \n",
    "\n",
    "So, now we can fit linear functions to data. This is great for all sorts of applications, but what if the real problem we're trying to model isn't linear? Lots of relationships that we can find the world are complicated and linear functions just won't cut it. In the next part of the exercise, we'll see how we can use linear regression to fit more complicated functions to our data. \n",
    "\n",
    "First, we'll need a non-linear source of data. We'll use polynomials to get started.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Implement polynomial_1D(w, x)\n",
    "def polynomial(w, x):\n",
    "    \"\"\"Compute w[0] + w[1] * x + w[2] * x^2 ... + w[n] * x^n\"\"\"\n",
    "    \"\"\"replace with your code\"\"\"\n",
    "    return 0\n",
    "\n",
    "poly_weights = np.array([2, 3, -2, .3]) / 10\n",
    "poly_w = lambda x: polynomial(poly_weights, x)\n",
    "plot_f(poly_w, x_lim=(-5, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Now that we can compute polynomials, we can create a noisy dataset --- just like we did before\n",
    "\n",
    "def generate_dataset(w_true, x_vals):\n",
    "    \"\"\"\n",
    "    Compute a list of noisy y values by first computing the polynomial \n",
    "    at each x in the list x_vals and then adding noise\n",
    "    \"\"\"\n",
    "    \"\"\"replace with your code\"\"\"\n",
    "    return 0\n",
    "\n",
    "## Let's sample some data and plot it\n",
    "\n",
    "## use data from the range (-5, 5) instead of (0, 1)\n",
    "N = 100\n",
    "x_vals_poly = 10 * (np.random.rand(N) - .5)\n",
    "noisy_y_vals_poly = generate_dataset(poly_weights, x_vals_poly)\n",
    "\n",
    "plot_f(poly_w, x_vals=x_vals_poly, y_vals=noisy_y_vals_poly, x_lim=(-5, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have data, we can try to recover the correct weights. The only problem is that the function we're using to generate data is in non-linear in x. First, we can just see what happens if we do linear regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Now we can try it with the noisy data\n",
    "loss = lambda x: dataset_loss(x[0], x[1], x_vals_poly, noisy_y_vals_poly)\n",
    "dloss = lambda x: d_dataset_loss_dw(x[0], x[1], x_vals_poly, noisy_y_vals_poly)\n",
    "\n",
    "(w_noisy, w0_noisy), l = gradient_descent(loss, dloss, w_init, 1e-3, 1e-8)\n",
    "\n",
    "plot_1D(w_noisy, w0_noisy, x_vals_poly, noisy_y_vals_poly, x_lim=(-5, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we can see is that linear regression does its best to match the data and balance out the errors, but it just can't represent the curves in the data. \n",
    "\n",
    "In order to deal with this, our first step will be to look at <it>multi-dimensional</it> regression. So far we have been dealing with data where x is one dimensionsal, but in linear regression we can actually handle the case of multi-dimensional inputs just fine. This means that <b>x = [x_1, x_2, ..., x_n]</b>. A linear function will be defined by a list of n+1 weights <b>w = [w_0, w_1, ..., w_n]</b>. Our linear function now multipies each weight with the corresponding x value and then adds them together with w_0. <b>y(x) = w_0 + w_1 * x_1 + w_2 * x_2 + ... w_n * x_n</b>.\n",
    "\n",
    "It is not hard to modify the gradient_descent function you wrote \n",
    "to handle high-dimensional weights. (Actually, you only need to change \n",
    "the gradient and loss functions.) However, doing it efficiently can be\n",
    "challenging. Tomorrow, we'll make use of efficient implementations of gradient descent. For the next section, we'll use the analytic solution to least-squares, which uses linear algebra to exactly compute the best line.\n",
    "\n",
    "You can ignore the <b>r</b> parameter for now --- we'll get to that shortly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def least_squares(x_vals, y_vals, r=0):\n",
    "    \"\"\"\n",
    "    Computes the exact solution to regularized least squares\n",
    "    w = (X'X + r*I)^-1 X'Y\n",
    "    \"\"\"\n",
    "    X = np.array(x_vals)\n",
    "    Y = np.array(y_vals)\n",
    "    N, d = X.shape\n",
    "    x_bar = np.mean(X, axis=0)\n",
    "    y_bar = np.mean(Y)\n",
    "    X_c = X - x_bar\n",
    "    Y_c = Y - y_bar\n",
    "    X = np.c_[np.ones(N), X]\n",
    "    w = np.linalg.solve(X_c.T.dot(X_c) + r * np.eye(d), X_c.T.dot(Y_c))\n",
    "    loss = np.linalg.norm(X_c.dot(w) - Y_c)/N\n",
    "    w0 = y_bar - x_bar.dot(w)\n",
    "    w = np.r_[w0, w]\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now that we can fit linear functions to high-dimensional data, we can use this to do polynomial regression! The trick is to recognize that fitting a polynomial function to one-dimensional data is the same as fitting a linear function to a very particular dataset. Instead of having a one dimensional input <b>x</b> we can replace it with a multi-dimensional input <b>[x, x^2, x^3, x^4,...]</b>. Once we've done this, we can fit weights to this data and we will have learned the coefficients of our polynomial! This trick, replacing <b>x</b> with a high-dimensional function of <b>x</b>, is called <it>featurization</it>. It is a common technique at the core of a lot of machine learning applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## First, we need to write a function to compute the high-dimensional features for a \n",
    "## dataset x\n",
    "def compute_poly_features(x_vals, n):\n",
    "    \"\"\"\n",
    "    Input: a list of one dimensional x values [x_i]\n",
    "    Output: a list of lists, where the ith list is the powers of x_i up to n\n",
    "            [x_i, x_i^2, ..., x_i^n]\n",
    "    \"\"\"\n",
    "    \"\"\"replace with your code\"\"\"\n",
    "    return 0\n",
    "\n",
    "## Now we can compute some polynomial features for our data\n",
    "x_vals_poly_nd = compute_poly_features(x_vals_poly, 3)\n",
    "## Once we have that, we're all set to do polynomial regression!\n",
    "\n",
    "w_inferred, l = least_squares(x_vals_poly_nd, noisy_y_vals_poly)\n",
    "\n",
    "poly_inferred = lambda x: polynomial(w_inferred, x)\n",
    "plot_f(poly_inferred, x_vals=x_vals_poly, y_vals=noisy_y_vals_poly, x_lim=(-5, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Alright, now we can fit polynomials to data! One important detail is that we knew what type of function the data was sampled from. We already knew that the polynomial was a third degree polynomial so we knew the right type of features to compute. Next, we'll try to figure out what the right dimension is, given only the data as input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## First, we'll take a look at how well different types of polynomial features do on the data above\n",
    "\n",
    "def fit_poly(x_vals, y_vals, n):\n",
    "    \"\"\"\n",
    "    Compute a polynomial fit to the data\n",
    "    Use a polynomial of degree n\n",
    "    Generate a plot of the inferred function\n",
    "    \n",
    "    Return a pair of (loss, weights)\n",
    "    \"\"\"\n",
    "    ## Hint: you should be able to adapt the code above\n",
    "    x_vals_poly = compute_poly_features(x_vals, n)\n",
    "    ## Once we have that, we're all set to do polynomial regression!\n",
    "    ## Make a function call to least_squares to fit the data\n",
    "    \"\"\"replace with your code\"\"\"\n",
    "    w_inferred, l = 0, 0\n",
    "    return w_inferred, l\n",
    "\n",
    "\n",
    "weights = []\n",
    "losses = []\n",
    "n_vals = [1, 3, 5, 9]\n",
    "for n in n_vals:\n",
    "    w_n, l_n = fit_poly(x_vals_poly, noisy_y_vals_poly, n)\n",
    "    print n, l_n\n",
    "    weights.append(w_n)\n",
    "    losses.append(l_n)\n",
    "    \n",
    "\n",
    "plt.subplots(1, 1)    \n",
    "plt.plot(n_vals, losses)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see an interesting phenomenon as we increase the degree of the polynomial that we fit. The loss always goes down as we make the polynomial more complicated. The story here is that the bigger polynomials can wiggle more --- so they do a better job of matching the wiggles in the data. Unfortunately, this isn't the whole story. We're measuring loss on the <it>training</it> data. What we want is an algorithm that does well on new data. If we look at the real polynomial that generated the data (with degree 3), and compare it with fit with degree 9, we can see that they are actually quite different in spots where we don't have much data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "poly_true = lambda x: polynomial(poly_w, x)\n",
    "plot_f(poly_w, x_lim=(-6, 6), ax=ax, color='b', x_vals=x_vals_poly, y_vals=noisy_y_vals_poly)\n",
    "\n",
    "poly_9 = lambda x: polynomial(weights[-1], x)\n",
    "plot_f(poly_9, x_lim=(-6, 6), ax=ax, color='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we can see is that the functions line up pretty well where there is lots of data, but very quickly they diverge in regions without data. This is called <b>overfitting</b> --- our function has the ability to wiggle a lot and so it is able to model small functuations in the data, noise. \n",
    "\n",
    "We can see an extreme version of this by trying to fit polynomials to very small amounts of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N=10\n",
    "x_vals = np.random.rand(N)\n",
    "w = [1, -3]\n",
    "y_vals = x_vals * w[1] + w[0] + np.random.normal(size=N)/2\n",
    "x_lim = [-.5, 1.5]\n",
    "f_true = lambda x: polynomial(w, x)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "plot_f(f_true, x_vals=x_vals, y_vals=y_vals, ax=ax, color='b', x_lim=x_lim)\n",
    "\n",
    "w_inferred, l_inferred = fit_poly(x_vals, y_vals, 5)\n",
    "\n",
    "f_inferred = lambda x: polynomial(w_inferred, x)\n",
    "plot_f(f_inferred, ax=ax, color='r', x_lim=x_lim)\n",
    "ax.set_xlim(x_lim)\n",
    "ax.set_ylim([-3, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we avoid this issue?\n",
    "\n",
    "The first thing we need to do is be able to figure out if we are overfitting in the first place. Just looking at performance on the training data isn't enough. As we make the possible functions more complicated, they can always do at least as well on the training set. Instead, we need a way to measure performance that the more expressive functions can't optimize on. \n",
    "\n",
    "One way to think about this is like studying for a math test in school. If you can see questions in advance, you can memorize all of the answers ahead of time. This lets you do well on the test, but you don't really learn the math. Instead, teachers will usually let you work on practice questions, but the test questions are new (although they are similar to the practice questions). We can do the same thing for linear regression. We take our dataset and we split it into two parts: a <it>training</it> set that we use to figure out good weights and a <it>validation</it> set that we use to figure out if those weights are overfitting. The training set behaves like the practice questions, and the validation set behaves like the actual test questions for our machine learning 'student'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Implement a function that takes as input a data set and splits it into a training set and a validation set\n",
    "def split_dataset(x_vals, y_vals, n_validate):\n",
    "    \"\"\"replace with your code\"\"\"\n",
    "    x_validate, y_validate, x_train, y_train = 0\n",
    "    return x_validate, y_validate, x_train, y_train\n",
    "\n",
    "def create_dataset(N):\n",
    "    x_vals = np.random.rand(N)\n",
    "    w = [1, -3]\n",
    "    y_vals = x_vals * w[1] + w[0] + np.random.normal(size=N)/2\n",
    "    x_lim = [-.5, 1.5]\n",
    "    return split_dataset(x_vals, y_vals, int(np.round(N/3)))\n",
    "\n",
    "x_validate, y_validate, x_train, y_train = create_dataset(15)\n",
    "\n",
    "def validate_loss(w):\n",
    "    l = 0\n",
    "    for x, y  in zip(x_validate, y_validate):\n",
    "        l += quad_loss(polynomial(w, x), y)\n",
    "    return l/len(y_validate)\n",
    "\n",
    "w_1, l_1 = fit_poly(x_train, y_train, 1)\n",
    "lv_1 = validate_loss(w_1)\n",
    "w_3, l_3 = fit_poly(x_train, y_train, 3)\n",
    "lv_3 = validate_loss(w_3)\n",
    "w_5, l_5 = fit_poly(x_train, y_train, 5)\n",
    "lv_5 = validate_loss(w_5)\n",
    "\n",
    "print \"Degree 1\\tTraining Loss: {} Validation Loss: {}\".format(l_1, lv_1)\n",
    "print \"Degree 3\\tTraining Loss: {} Validation Loss: {}\".format(l_3, lv_3)\n",
    "print \"Degree 5\\tTraining Loss: {} Validation Loss: {}\".format(l_5, lv_5)\n",
    "\n",
    "degree_vals = [1, 3, 5]\n",
    "training_losses = [l_1, l_3, l_5]\n",
    "validation_losses = [lv_1, lv_3, lv_5]\n",
    "plt.plot(degree_vals, training_losses, label='training losses')\n",
    "plt.plot(degree_vals, validation_losses, label='validation losses')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should run the cell above a couple of times. There are two patterns that you should see. \n",
    "\n",
    "First, the training loss always decreases from Degree 1 to Degree 3 and again from Degree 3 to Degree 5 --- this is because those functions are overfitting. There are more options for the optimization to choose from, so it does better on the training examples.\n",
    "\n",
    "Second, the validation loss should usually increase from the bottom to the top. Occasionally, this won't be the case due to random chance but for the most part, the validation loss should increase as we make the model more complicated than the true source of the data. \n",
    "\n",
    "If we do this with more data though, we will see that these patterns largely go away. This is because there's enough data that the noise in the training set averages out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_validate, y_validate, x_train, y_train = create_dataset(500)\n",
    "\n",
    "def validate_loss(w):\n",
    "    l = 0\n",
    "    for x, y  in zip(x_validate, y_validate):\n",
    "        l += quad_loss(polynomial(w, x), y)\n",
    "    return l/len(y_validate)\n",
    "\n",
    "w_1, l_1 = fit_poly(x_train, y_train, 1)\n",
    "lv_1 = validate_loss(w_1)\n",
    "w_3, l_3 = fit_poly(x_train, y_train, 3)\n",
    "lv_3 = validate_loss(w_3)\n",
    "w_5, l_5 = fit_poly(x_train, y_train, 5)\n",
    "lv_5 = validate_loss(w_5)\n",
    "\n",
    "print \"Degree 1\\tTraining Loss: {} Validation Loss: {}\".format(l_1, lv_1)\n",
    "print \"Degree 3\\tTraining Loss: {} Validation Loss: {}\".format(l_3, lv_3)\n",
    "print \"Degree 5\\tTraining Loss: {} Validation Loss: {}\".format(l_5, lv_5)\n",
    "\n",
    "degree_vals = [1, 3, 5]\n",
    "training_losses = [l_1, l_3, l_5]\n",
    "validation_losses = [lv_1, lv_3, lv_5]\n",
    "plt.plot(degree_vals, training_losses, label='training losses')\n",
    "plt.plot(degree_vals, validation_losses, label='validation losses')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we can measure overfitting, we can try to deal with it. In some cases, we can directly change the model class to regularize. For example, we could search to find the degree of polynomial that does well for the validation loss. This is OK, but doesn't work everywhere. In modern machine learning applications we sometimes have 1000 (!) dimensional inputs --- this often means we can overfit even with linear functions.\n",
    "\n",
    "Instead, we'll need a more general way to control for overfitting. We can get a hint by looking at the weights that we compute when we overfit. Run the next cell a couple of times to look for patterns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_validate, y_validate, x_train, y_train = create_dataset(15)\n",
    "\n",
    "w_1, l = fit_poly(x_train, y_train, 1)\n",
    "w_3, l = fit_poly(x_train, y_train, 3)\n",
    "w_5, l = fit_poly(x_train, y_train, 5)\n",
    "\n",
    "print \"Degree 1 Weights: {}\".format(w_1)\n",
    "print \"Degree 3 Weights: {}\".format(w_3)\n",
    "print \"Degree 5 Weights: {}\".format(w_5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you should see is that the weights that overfit have very large magnitudes. So, one way to control for overfitting is to create a penalty for using big weights. This way the optimization will have to choose: if it helps to explain the data, then it will incur a cost for large weights; but, if it is possible to model the data with small weights, then the optimization will choose that. This technique is called <it>regularization</it>. \n",
    "\n",
    "If we go back to the definition of least_squares, this is what the parameter <b>r</b> is for. Our optimization is going to trade off between the training loss and the sum of squared weights: <b>w_0^2 + w_1^2 + ... w_n^2</b>. <b>r</b> controls the trade-off between these two terms. If <b>r</b> is large, then the optimization will focus on keeping the weights small; if <b>r</b> is small, then the optimization will focus on fitting the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_validate, y_validate, x_train, y_train = create_dataset(30)\n",
    "x_train_poly = compute_poly_features(x_train, 15)\n",
    "\n",
    "def validate_loss(w):\n",
    "    l = 0\n",
    "    for x, y  in zip(x_validate, y_validate):\n",
    "        l += quad_loss(polynomial(w, x), y)\n",
    "    return l/len(y_validate)\n",
    "\n",
    "r_vals = [1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3]\n",
    "loss_t = []\n",
    "loss_v = []\n",
    "w_squared = []\n",
    "for r in r_vals:\n",
    "    w, l = least_squares(x_train_poly, y_train, r=r)\n",
    "    loss_t.append(l)\n",
    "    loss_v.append(validate_loss(w))\n",
    "    w_squared.append(np.sqrt(np.linalg.norm(w[1:])))\n",
    "fig, ax = plt.subplots(2, 1, sharex=True)\n",
    "plt.sca(ax[0])\n",
    "plt.plot(-np.log(r_vals), loss_t, label='training loss')\n",
    "plt.plot(-np.log(r_vals), loss_v, label='validation loss')\n",
    "plt.legend()\n",
    "plt.sca(ax[1])\n",
    "plt.plot(-np.log(r_vals), w_squared, label='regularization cost')\n",
    "plt.legend()\n",
    "ax[1].set_xlabel('-log r')\n",
    "fig.set_figheight(10)\n",
    "\n",
    "#print loss_t\n",
    "#print loss_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can try out what we've learned on some real data! \n",
    "\n",
    "In this case, we'll be using one of the datasets from the <a href=\"http://archive.ics.uci.edu/ml/index.php\">UCI Machine Learning Repository</a>. They have a variety of real world datasets that you can use to try out machine learning algorithms. \n",
    "\n",
    "We'll be looking at a prediction problem related to <a href=\"http://archive.ics.uci.edu/ml/datasets/Energy+efficiency\">energy efficiency</a>. One of the biggest energy costs in a home comes from heating and cooling. Where a house is built can change the heating and cooling costs a lot. For example, a house built largely in the shade away from direct sunlight will cost less to air condition in the summer. \n",
    "\n",
    "Your job is to build a tool that allows architects to figure out the heating and cooling costs based on some features of a proposed home design. You have access to 8 features:\n",
    "\n",
    "X1\tRelative Compactness\n",
    "\n",
    "X2\tSurface Area \n",
    "\n",
    "X3\tWall Area \n",
    "\n",
    "X4\tRoof Area \n",
    "\n",
    "X5\tOverall Height \n",
    "\n",
    "X6\tOrientation \n",
    "\n",
    "X7\tGlazing Area \n",
    "\n",
    "X8\tGlazing Area Distribution\n",
    "\n",
    "Our goal will be to predict the cooling load. This will tell us how expensive cooling this particular house is likely to be. We've collected 100 training examples and your job will be to train a predictor for these two values. \n",
    "\n",
    "We'll start by predicting the heating load. We've written code to load the data and compute the training loss. You need to figure out our to regularize our prediction --- experiment with different choices to find the best value of <b>r</b>. Try out a couple of different values in order to get a better validation loss. You'll need to try a big range of values. Consider values for <b>r</b> in the range [1e6, 1e-6]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Load in the data\n",
    "X, Y = load_ee_training()\n",
    "\n",
    "## Split the data in to train and validation sets\n",
    "X_validate, Y_validate, X_train, Y_train  = split_dataset(X, Y, int(len(Y)/5))\n",
    "\n",
    "def predict(w, x):\n",
    "    return w.dot(np.r_[1, x])\n",
    "\n",
    "def training_loss(w):\n",
    "    l = 0\n",
    "    for x, y in zip(X_train, Y_train):\n",
    "        l += quad_loss(predict(w, x), y)\n",
    "    return l/len(Y_train)\n",
    "\n",
    "def validate_loss(w):\n",
    "    l = 0\n",
    "    for x, y  in zip(X_validate, Y_validate):\n",
    "        #print predict(w, x), y, quad_loss(predict(w, x), y)\n",
    "        l += quad_loss(predict(w, x), y)\n",
    "    return l/len(Y_validate)\n",
    "\n",
    "## Replace with your value for r\n",
    "r = 1e-6\n",
    "w, _ = least_squares(X_train, Y_train, r = r)\n",
    "lt = training_loss(w)\n",
    "lv = validate_loss(w)\n",
    "print \"Training Loss:\\t{}\\tValidation Loss:\\t{}\".format(lt, lv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try the above section out for a variety of values for <b>r</b> above. Your goal is to find a value that gets a low validation loss. \n",
    "\n",
    "Once you're happy with that, you can run the next cell to verify your performance on the <b>test set</b>. This is the final set of data that we'll use to evaluate our learning algorithm. This is where we pretend that we are getting new data entirely and so we can only evaluate on it once. (Otherwise, we might somehow cheat and use our hyperparameters to 'overfit' to the testing set!) You can run the next cell once to see how well you've tuned your linear regression. \n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Train on the whole dataset\n",
    "w, lt = least_squares(X_train, Y_train, r = r)\n",
    "\n",
    "l = 0\n",
    "for i in range(len(X_train)):\n",
    "    l += np.power(predict(w, X_train[i]) - Y_train[i], 2)/len(X_train)\n",
    "\n",
    "testing_loss = ee_evaluate(w)\n",
    "print \"Testing Loss:\\t{}\".format(testing_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
